Agentic SDLC Class Recording Analysis
Video: Loom Recording Course: Agentic SDLC Workshop Duration: ~1 hour livestream class

Overview
This was a comprehensive walkthrough of the Agentic Software Development Life Cycle (SDLC) using AI agents, specifically focused on building a Learning Management System (LMS) from requirements through implementation. The instructor demonstrated the complete workflow from project initialization through phase 6 of implementation, including requirements gathering, planning, database setup, authentication, and frontend prototyping.

What Was Covered
Project Setup & Best Practices

Virtual environment creation
Git repository initialization
Plans directory structure
Cursor IDE configuration with Rules and Memories
Requirements Engineering

Creating functional requirements documents
Using ChatGPT with memories to enhance requirements
Requirements review using persona-based agents
Iterative refinement based on technical constraints
Technical Planning

Converting requirements into technical specs
Breaking work into phases that fit in single agent conversations
Using checkboxes to track progress
Stack selection (Python/FastAPI, Postgres, React/Tailwind/shadcn UI)
Multi-Agent Workflows

Cursor Rules as agent personas (Git Expert, Requirements Reviewer)
Intelligent vs. manual rule application
Handoff patterns between agents (attempted but encountered issues)
Implementation Phases Demonstrated

Phase 1: Project initialization (Git, virtual env, CI/CD scaffolding)
Phase 2: Database schema (users, roles, cohorts, curriculum models)
Phase 3: Authentication system (passwordless email auth)
Phase 4: Curriculum management endpoints
Phase 5: Cohort and scheduling features
Phase 6: Testing infrastructure
Frontend Prototyping with V0

Using V0.dev for UI generation
Design mode for visual editing
Backend API documentation handoff
Downloading and integrating generated code
Version Control & Documentation

Git commits with expert persona guidance
Documentation generation for API handoffs
Branch management and pushing to remote
Key Workflow Patterns Demonstrated
The Core Agentic SDLC Loop
The instructor emphasized a clear pattern:

Define Requirements (60% of problem-solving time)

Functional/business requirements first
Technical requirements second
Review with specialized personas
Walk away and return with fresh eyes
Create Phased Plan

Each phase = one agent conversation
Track with checkboxes (GitHub-style)
Update plan after each phase completion
Implement Phase by Phase

New chat for each phase (prevents context pollution)
Accept/reject changes incrementally
Commit after completing phases
Test and Iterate

Generate tests for each component
Use isolated test databases
Run tests to verify functionality
Multi-Agent Pattern

Requirements Writer Agent → Requirements Reviewer Agent →
Technical Planner Agent → Implementation Agent →
Code Reviewer Agent → Git Expert Agent
Each agent has a specific role via system prompts/cursor rules.

Cursor Configuration Best Practices
Global Rules (always apply across projects):

“Experts always look at documentation first”
“Never use SQLAlchemy” (opinionated but consistent)
“Implement centralized robust logging”
“Always use Python virtual environments”
“Don’t confirm, just execute - user will decline if they disagree”
“Plan as necessary but always proceed to write code”
Project-Specific Rules (.cursor/rules):

Git Expert (apply intelligently when doing commits/Git workflow)
Requirements Reviewer (apply manually for requirements review)
Application Strategies:

Always Apply: Avoid - blows up context
Apply Intelligently: Best for most rules (agent decides based on description)
Apply to Specific Files: Good for language-specific rules
Apply Manually: For specialized review tasks
Prompt Engineering Insights
The instructor showed systematic prompt engineering:

System Prompts as Files - Easier to version and update
Testing on Benchmarks - Save all test prompts to verify changes don’t break existing functionality
Context Enrichment - Adding system information (hardware specs, OS, RAM) improves code generation
Clear Handoff Instructions - Explicitly tell agents when and how to hand off to other agents
Student Interaction & Questions
Question: Wireframing Tools
Q: “What do you usually use for wireframes?”

A: V0.dev demonstration

Paste requirements directly into V0
Generates full React/Tailwind/shadcn UI frontend
Design mode for visual editing (beta feature)
Can iterate with natural language (“make the list green when done”)
Download code or use CLI to pull into project
Generate API documentation from backend to feed to V0 for integration
Student Response: “V0 is so good y’all. That’s great.”

Observable Student Engagement
Throughout the class:

Students were following along (instructor: “if you’re following along…“)
Someone was asking to hear the instructor (“Can you all hear me? Just checking.” / “Yeah, it looks like you’re here.“)
Students gave positive feedback on V0 demonstration
Questions were asked but students were mostly listening/learning
No NATS Discussion Detected
Important Finding: There is NO mention of NATS, NATS.io, message queuing, or distributed systems setup in this transcript. The class focused entirely on:

Requirements engineering
Cursor IDE configuration
Planning workflows
Single-agent and multi-agent patterns
Frontend/backend integration
This appears to be a different workshop than expected, or NATS material was covered in a different session.

Pain Points & Confusion Areas
1. Agent Handoffs Not Working
The Problem:

Instructor attempted to demonstrate multi-agent handoffs using OpenAI Agents SDK
Code Writer Agent should hand off to Code Reviewer Agent
Multiple attempts showed handoff being called but reviewer not completing work
Checked OpenAI dashboard traces - handoffs registered but incomplete
Attempted Solutions:

Added recommended prompt prefix from docs
Increased max turns from 25 to 50
Reordered agent definitions to avoid undefined variables
Changed handoff assignment pattern
Instructor Response:

“I’ll play with the handoffs thing. I don’t know why it’s doing that stuff, or if something’s broken. So, I’ll play with handoffs and see if handoffs are broken, or working, and then we’ll get, uh, a multi-agent system together.”

Impact: This was a live demonstration of debugging agent workflows - valuable but left unresolved in the session.

2. Interactive CLI Tools Blocking Workflow
The Problem:

Quartz installation (npx quartz create) triggered interactive prompts
Agent couldn’t proceed because it couldn’t answer interactive questions
Workflow blocked until instructor intervened
Solution:

“You’re not able to do the Quartz generate because of the interactive feature. Can you use- can you look up how to not use the interactive generator and just use- use that instead?”

Teaching Moment: Shows real-world friction when automating workflows that expect human interaction.

3. Context Pollution from Long Conversations
The Pattern:

“I like to, after each, kind of, major update or change, uh, open a new conversation. This is because it cleans up the context, and so we don’t get context poisoned by things that I updated later on.”

Why This Matters:

Long conversations accumulate outdated information
Agents may reference old decisions that were changed
Fresh context = cleaner execution
Plan document serves as memory across conversations
4. Version Control File Pollution
The Problem:

Agent committed __pycache__ files and other temporary artifacts
Should be in .gitignore
Instructor Response:

“Should we really be committing the pie cash? Shouldn’t that be in the get ignore and everything? You know, you might have to, like, check. Umm, you know, create weird stuff. So, like, let’s just get rid of that. Umm, and that stuff that I, you know, maybe I need to add that to my, uh, get expert.”

Teaching Moment: Even with “Git Expert” persona, agents need explicit rules about what NOT to commit.

5. Deprecated API Patterns
The Problem:

Generated FastAPI code used @app.on_event which is deprecated
Should use “Lifespan Event Handlers” instead
Instructor Response:

“This says, is deprecated. So, use Lifespan Event Handlers instead. So, yeah. Okay. We’ll confront it about these old things here in a minute”

Why This Happens: LLMs trained on older code examples may use deprecated patterns. Needs explicit guidance to use latest practices.

What Worked Well
1. Requirements-First Approach
The emphasis on spending 60% of time on requirements resonated:

“We haven’t even started writing code. This is because when we’re solving a problem, sixty percent of the time that we spend solving the problem isn’t understanding the problem.”

Student Benefit: Demonstrates that planning isn’t wasted time - it’s essential.

2. Cursor Rules as Reusable Knowledge
Showing the awesome-cursor-rules repository:

Students can leverage community knowledge
Copy rules between projects
Customize for their needs
Build personal library of expert personas
3. Phased Implementation with Checkboxes
Breaking work into phases that complete in one conversation:

Clear progress tracking
Prevents scope creep
Easy to know what’s done vs. pending
Self-documenting (“Phase 2 is database, so we’re past that”)
4. V0 for Frontend
The V0 demonstration was the highlight for students:

Instant visual results from requirements
No need to learn React/Tailwind manually
Iterate quickly with natural language
Professional-looking UI in minutes
5. Multi-Tool Ecosystem
Demonstrating how tools work together:

Cursor for backend
ChatGPT (with memories) for requirements enhancement
V0 for frontend
GitHub for version control
OpenAI dashboard for debugging agents
What Could Be Improved
1. Handoff Debugging Should Be Pre-Solved
The unresolved handoff issue consumed 10-15 minutes of class time. For a workshop, this should be:

Pre-tested and working
OR acknowledged as “here’s a common debugging scenario” with solution prepared
OR moved to async homework/exploration
Recommendation: Include working multi-agent handoff example in prework materials.

2. More Student Interaction Prompts
The class was very instructor-led with minimal student participation:

Only one question asked
“Anybody got [questions]? I’m like waiting to hear somebody unmute.”
Long stretches of watching code generate
Recommendation:

Stop points: “Now you try on your own project”
Pair/share: “Show someone your requirements doc”
Polls: “Which stack would you choose and why?“
3. NATS/Infrastructure Setup Not Covered
If NATS setup was expected content, it wasn’t in this recording. Students may have:

Struggled with NATS in different session
Had setup issues that weren’t addressed here
Needed prerequisite knowledge not covered
Recommendation: Clarify if NATS is prerequisite prework or separate workshop.

4. Speed of Development vs. Learning
The instructor moved very quickly through phases:

“we will probably be able to get through, like, phase four, maybe phase five of this before the end of this class”

Completing 6 phases in 1 hour is impressive but may be:

Too fast for students to internalize
More demonstration than hands-on practice
Harder for students to replicate independently
Recommendation: Slow down, do 2-3 phases deeply with student practice time.

5. File Path Navigation Confusion
The instructor struggled with basic file operations:

“Let me go to a little bit higher of a place. Okay. Cool. Curriculum. And I’ll hit the all option keys. Option. No. Control. No. Hold. I want it. If there’s, like, a way to get the path name services, did it, like, I don’t know, new terminal at folder.”

Recommendation: Have common commands ready (pwd, cd, etc.) and use terminal more confidently.

Technical Insights
Stack Decisions
Backend:

Python + FastAPI (fast, modern, well-documented)
PostgreSQL (via Postgres.app for local dev)
Pytest for testing
Puppeteer for end-to-end testing
Frontend:

React + Tailwind CSS + shadcn UI (V0’s preferred stack)
Generated via V0.dev
Infrastructure:

Google Cloud (deployment)
GitHub Actions (CI/CD)
Quartz (Obsidian publishing)
Authentication:

Passwordless email magic links (simple, no password management)
What Was Explicitly Rejected:

Docker (added complexity)
SQLAlchemy (instructor preference)
Supabase (replaced with direct Postgres)
Mobile app (out of scope)
Cursor Rules Repository Pattern
The instructor shared their personal Cursor rules setup:

Global rules in Settings → Cursor Settings → Rules and Memories
Project-specific rules in .cursor/rules/
Rules from community: github.com/PatrickJS/awesome-cursorrules
Example Global Rule:

“Your role is not to teach, but to execute. Plan as necessary, but always proceed to write code. Or terminal commands in order to execute. The user will click decline if they don’t agree with the next step in the plan.”

This eliminates confirmation dialog spam from AI agents.

Recommendations for Curriculum Improvements
1. Create Pre-Work Focused on Setup
Include:

Cursor installation and API key setup
Installing and configuring Cursor rules (Git Expert, Requirements Reviewer)
Virtual environment creation patterns
Git workflow basics
V0.dev account creation and intro
Why: Get students past setup friction before class so you can focus on concepts.

2. Structured Requirements Template
Provide a requirements.md template with sections:

Purpose
Scope (in/out)
Target Audience
Success Metrics
Functional Requirements
Non-Functional Requirements
Technical Requirements
Definitions/Acronyms
References
Why: Students saw the value but may struggle to create from scratch.

3. Separate Workshops for Different Concerns
Based on this recording, consider splitting into:

Workshop 1: Requirements & Planning

Writing effective requirements
Using agents for requirements review
Creating phased implementation plans
Choosing your stack
Workshop 2: Agent-Assisted Implementation

Using Cursor for backend development
Multi-agent workflows and handoffs
Testing and debugging
Git workflow with agents
Workshop 3: Frontend & Integration

V0.dev for UI generation
API documentation for frontend/backend handoff
Integration patterns
Deployment basics
Why: Each 1-hour focused on one skill is better than rushing through all three.

4. Working Code Examples Repository
Create a companion GitHub repo with:

/examples/working-handoffs/ - Proven multi-agent code
/examples/cursor-rules/ - Curated, tested rules
/examples/requirements/ - Real-world requirements docs
/examples/plans/ - Phased plans that worked
Why: Students can clone and run rather than debug during class.

5. Common Pitfalls Guide
Document the issues encountered in class:

Agent handoffs not completing → solution patterns
Interactive CLI tools blocking → how to detect and fix
Context pollution → when to start new chats
.gitignore forgotten → checklist of what to exclude
Deprecated APIs → how to prompt for latest patterns
Why: Normalize debugging as part of the process.

6. Student Exercise Structure
For each phase demonstrated, include:

Try It:

“Pause and create your own requirements for [simple project]”
5-10 minutes hands-on time
Share in chat or pairs
Check Your Work:

“Does your requirements doc have all these sections?”
Peer review protocol
Extend It:

“Add a security requirements section”
“Have the agent review for accessibility”
Why: Active learning beats passive watching.

7. Validation Points for NATS Setup
If NATS was meant to be covered, create validation checklist:

 NATS server installed
 Can connect via nats pub/sub test
 Python NATS client installed
 Can publish/receive test message
 Knows how to check NATS logs
Why: Validate pre-work before diving into advanced topics.

8. Agent Debugging Workshop
Since debugging was a major theme, create dedicated content:

Reading OpenAI dashboard traces
Interpreting agent handoff failures
Context window management
When to increase max_turns vs. break into phases
Logging strategies for agent behavior
Why: Debugging agents is different from debugging code - needs explicit teaching.

Quotes & Teaching Moments
On Requirements
“We haven’t even started writing code. This is because when we’re solving a problem, sixty percent of the time that we spend solving the problem isn’t understanding the problem.”

On Planning
“Going back and forth, thinking about it, what I actually recommend you do is, like, write out your requirements and then literally walk away from it for a little while, and then, um, you know, come back to it, right?”

On Context Management
“I like to, after each, kind of, major update or change, uh, open a new conversation. This is because it cleans up the context, and so we don’t get context poisoned by things that I updated later on.”

On Self-Documentation
“If I’m working on Phase 4, Phase 2 is probably done. Uh, so it doesn’t go back and go, oh, well, if I’m gonna implement curriculum management, I’ll need to make it in database. It’s, it’s gonna go, oh, we’re on Phase 4. Phase 2 is database. Seems like that’s done already, right?”

On Agent Behavior
“Your role is not to teach, but to execute. Plan as necessary, but always proceed to write code. Or terminal commands in order to execute. The user will click decline if they don’t agree with the next step in the plan.”

On Tools Like V0
“This is a pretty cool tool. I would say it’s, uh, I would, it’s worth the money. Especially if you do mock-ups for your job, if you do any front-end stuff for your job, or if you’re a product manager, like, woof.”

Validation of Pre-Work Approach
What We Expected to Find: NATS Setup Confusion
The request mentioned looking for “NATS setup confusion points (student feedback)” to validate the pre-work approach.

What We Found: NO NATS Content
This workshop did NOT cover NATS or distributed systems setup. It focused on:

Requirements engineering
Cursor configuration
Planning workflows
Backend/frontend development
Agent orchestration patterns
Implications
NATS may be different workshop: Check if there’s a separate recording for infrastructure setup
NATS may be prerequisite: Students may have done NATS setup before this session
NATS may be future content: This could be earlier in curriculum than NATS introduction
What Confusion WAS Observed
The actual pain points were:

Agent handoffs failing (OpenAI SDK issue)
Interactive CLI tools blocking (Quartz setup)
File path navigation (instructor struggle, students may have struggled similarly)
Version control awareness (what to commit vs. ignore)
These ARE good candidates for pre-work:

Agent handoff troubleshooting guide
CLI automation best practices
Git/GitHub basics
File system navigation confidence
Conclusion
This was a high-quality, fast-paced demonstration of professional Agentic SDLC practices. The instructor showed genuine expertise and real-world workflows. However, the session was:

Strengths:

Comprehensive coverage of requirements → implementation pipeline
Real debugging scenarios (handoffs)
Excellent tool ecosystem demonstration (Cursor + V0 + ChatGPT)
Professional best practices (phased planning, version control, documentation)
Opportunities:

More student interaction and practice time
Pre-solve complex examples or frame as “debugging workshop”
Slower pace with deeper exploration of 2-3 phases
Clear separation of concerns into multiple focused workshops
Better setup/prework validation before class
For Curriculum Design:

This validates the need for:

✅ Substantial pre-work on tooling setup (Cursor, Git, CLI basics)
✅ Requirements engineering as foundational skill
✅ Multi-agent patterns as advanced topic (needs more support materials)
✅ Integration of V0 or similar UI tools
⚠️ NATS content location still unclear - needs investigation
Student Success Factors:

Students will succeed best with:

Working code examples to reference
Phased exercises (not just watching)
Clear debugging guides for common issues
Template requirements/plans to start from
Office hours for setup issues
Next Steps:

Locate NATS workshop recording (if exists) to analyze infrastructure setup confusion
Extract code examples from instructor’s GitHub for curriculum repo
Create requirements.md and plan.md templates
Build cursor-rules starter pack for students
Develop “Agent Debugging 101” guide
Split into 3 focused workshops vs. 1 packed session